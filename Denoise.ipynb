{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z3DwlxO7nQAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e038564b-fb42-400c-c1d7-3d51144d939a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed, Embedding, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Parameters for the model\n",
        "embedding_dim = 100\n",
        "latent_dim = 54\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 1000\n",
        "max_length = 100\n",
        "epochs = 98\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=vocab_size)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Create a reverse word index\n",
        "reverse_word_index = {value + 3: key for key, value in word_index.items()}\n",
        "reverse_word_index[0] = '<PAD>'\n",
        "reverse_word_index[1] = '<START>'\n",
        "reverse_word_index[2] = '<UNK>'\n",
        "reverse_word_index[3] = '<UNUSED>'\n",
        "\n",
        "# Convert sequences back to text\n",
        "train_texts = [[reverse_word_index.get(i, '<UNK>') for i in sequence] for sequence in x_train]\n",
        "test_texts = [[reverse_word_index.get(i, '<UNK>') for i in sequence] for sequence in x_test]\n",
        "all_texts = train_texts + test_texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_Wv48dbTAQip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26.4\n",
        "!pip install --upgrade --force-reinstall gensim\n",
        "!pip install --upgrade --force-reinstall tensorflow\n"
      ],
      "metadata": {
        "id": "6VmFdf0DAQ5-",
        "outputId": "f00bb03e-2164-499a-d7df-d485665e42a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wrapt"
                ]
              },
              "id": "c55d3110e2dc451e940cb8292676b814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools (from tensorflow)\n",
            "  Downloading setuptools-80.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rich (from keras>=3.5.0->tensorflow)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow)\n",
            "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow)\n",
            "  Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.0.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.4/410.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, pygments, protobuf, packaging, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, optree, ml-dtypes, markdown-it-py, h5py, google-pasta, astunparse, tensorboard, rich, keras, tensorflow\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.0.9\n",
            "    Uninstalling namex-0.0.9:\n",
            "      Successfully uninstalled namex-0.0.9\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=all_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=20)\n",
        "\n",
        "# Convert the word2vec model to a dictionary\n",
        "# Create an embedding matrix where each row index corresponds to a word index\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        try:\n",
        "            embedding_vector = word2vec_model.wv[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            # Words not found in the embedding index will be all zeros\n",
        "            pass\n"
      ],
      "metadata": {
        "id": "DZCUH0cXrG6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sequence_to_text(sequence):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "\n",
        "def add_noise_to_text(sequence, noise_factor=0.1):\n",
        "    noisy_sequence = []\n",
        "    for i in sequence:\n",
        "        if random.random() < noise_factor:\n",
        "            # Randomly replace a word with another word\n",
        "            noisy_sequence.append(random.randint(1, vocab_size - 1))\n",
        "        else:\n",
        "            noisy_sequence.append(i)\n",
        "    return noisy_sequence\n",
        "\n",
        "# Add noise to the data\n",
        "noisy_x_train = [add_noise_to_text(seq) for seq in x_train]\n",
        "noisy_x_test = [add_noise_to_text(seq) for seq in x_test]\n",
        "\n",
        "# Convert to padded sequences\n",
        "noisy_x_train_padded = pad_sequences(noisy_x_train, maxlen=max_length, padding='post')\n",
        "noisy_x_test_padded = pad_sequences(noisy_x_test, maxlen=max_length, padding='post')\n",
        "\n",
        "# Example of noisy text\n",
        "print(\"Original:\", sequence_to_text(x_train[0]))\n",
        "print(\"Noisy:\", sequence_to_text(noisy_x_train[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvyLcd2rJMP",
        "outputId": "3333d85d-f409-4a1c-c5c0-0a82755991c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ? in with i like horrible business ? ? would killer ? which ? <START> going at fun <UNK> film make like lame character has novel ? a all final sense <UNK> real <START> find character nothing ? second perhaps they <START> find ? ? this city an br overall <START> horror has i <UNUSED> should ? was in with <START> ? ? despite <START> with their people is i like horrible an well it br ? <START> with this genre this is i taken that ? <UNK> she sex is and house and after <UNK> <START> ? ? i final which ? be <START> does is i an annoying <UNK> film where if at man it's film ? be <UNUSED> with is comedy you than some <UNK> in perfect i get ? and <START> think plot ? it fun <START> ? the ? <UNK> sequence at their like horrible wanted on getting night just the <START> ? ? br any other <START> couple it someone then he ? more on why <UNUSED> can't ? that <START> family with for still wanted on final <UNK> such his ? that if at you interesting how film any <START> family would i an ? other is i once <UNK> i ? seen could he it i ? was every he\n",
            "Noisy: ? in with i like horrible third ? ? would killer ? which ? <START> going at fun <UNK> film make like lame character has christmas ? a all final sense <UNK> hope <START> find john nothing ? second perhaps they <START> find ? ? this city an made overall <START> horror has i <UNUSED> should ? was in with imdb ? ? despite <START> with their people is i like horrible an well it br ? <START> with this genre it is i taken that ? <UNK> she sex is and house and after <UNK> <START> ? ? i final which ? figure <START> does is i an annoying <UNK> film where if at man it's film ? be <UNUSED> with is comedy french than some <UNK> in makes i get ? and see think real ? it fun <START> ? the ? <UNK> sequence at their like horrible wanted on getting night just the <START> ? ? br any other happened gets it someone then he ? starts on why <UNUSED> can't ? that <START> family with for still wanted dr final <UNK> they're his ? that if at you interesting how film hope <START> family would i an ? other is i once <UNK> i ? seen could he it i ? was every he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "input_text = Input(shape=(max_length,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(input_text)\n",
        "encoder_output = Bidirectional(LSTM(2*latent_dim, return_sequences=False))(encoder_embedding)\n",
        "encoder_output = Dropout(0.15)(encoder_output)\n",
        "encoder_output = Dense(latent_dim, activation='relu')(encoder_output)\n",
        "\n",
        "# Decoder\n",
        "decoder_input_seq = Input(shape=(max_length - 1,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(decoder_input_seq)\n",
        "decoder_input = RepeatVector(max_length)(encoder_output)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True)(decoder_input)\n",
        "decoder_lstm = Dropout(0.15)(decoder_lstm)\n",
        "decoder_lstm = LSTM(2*latent_dim, return_sequences=True)(decoder_lstm)\n",
        "decoder_output = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_lstm)\n",
        "\n",
        "\n",
        "# Compile and train with MSE loss\n",
        "autoencoder = Model([input_text, decoder_input_seq], decoder_output)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0), loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# # First, pad the sequences to max_length\n",
        "x_train_padded = pad_sequences(x_train, maxlen=max_length, padding='post')\n",
        "x_test_padded = pad_sequences(x_test, maxlen=max_length, padding='post')\n",
        "\n",
        "# y_train and y_test are simply the padded original sequences\n",
        "y_train = x_train_padded\n",
        "y_test = x_test_padded\n",
        "\n",
        "# Prepare decoder_input_data by shifting the sequences by one position\n",
        "decoder_input_train = np.zeros((x_train_padded.shape[0], max_length - 1), dtype=int)\n",
        "decoder_input_test = np.zeros((x_test_padded.shape[0], max_length - 1), dtype=int)\n",
        "\n",
        "for i in range(len(x_train_padded)):\n",
        "    decoder_input_train[i] = x_train_padded[i, 1:]  # Drop the first word\n",
        "\n",
        "for i in range(len(x_test_padded)):\n",
        "    decoder_input_test[i] = x_test_padded[i, 1:]  # Drop the first word\n"
      ],
      "metadata": {
        "id": "SlepF8SPnTqj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "uk5PGZXmSsmO",
        "outputId": "3228cb10-bf90-4945-f1f5-00f3f4e9dfed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │    \u001b[38;5;34m100,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m216\u001b[0m)       │    \u001b[38;5;34m180,576\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m216\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m)        │     \u001b[38;5;34m11,718\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m54\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m54\u001b[0m)   │     \u001b[38;5;34m23,544\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m54\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m108\u001b[0m)  │     \u001b[38;5;34m70,416\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1000\u001b[0m) │    \u001b[38;5;34m109,000\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">180,576</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">11,718</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,544</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">70,416</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">109,000</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m495,254\u001b[0m (1.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">495,254</span> (1.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m395,254\u001b[0m (1.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">395,254</span> (1.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m100,000\u001b[0m (390.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> (390.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def custom_lr_scheduler(epoch, lr):\n",
        "    # Decrease learning rate by 0.1 factor every 5 epochs\n",
        "    if epoch % 5 == 0 and epoch != 0:\n",
        "        lr = lr * 0.1\n",
        "    return lr\n",
        "\n",
        "# Define the callback\n",
        "lr_scheduler = LearningRateScheduler(custom_lr_scheduler)\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit([noisy_x_train_padded, decoder_input_train], np.expand_dims(y_train, -1),\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=([noisy_x_test_padded, decoder_input_test], np.expand_dims(y_test, -1)),\n",
        "          callbacks=[early_stopping, lr_scheduler])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tFDZGpOSrSt",
        "outputId": "b678eec7-2072-4efe-94a2-46404aec5be3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 45ms/step - loss: 4.8980 - val_loss: 4.8065 - learning_rate: 0.0100\n",
            "Epoch 2/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 44ms/step - loss: 4.7909 - val_loss: 4.8234 - learning_rate: 0.0100\n",
            "Epoch 3/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 4.7838 - val_loss: 4.8033 - learning_rate: 0.0100\n",
            "Epoch 4/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - loss: 4.7877 - val_loss: 4.8023 - learning_rate: 0.0100\n",
            "Epoch 5/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 4.7856 - val_loss: 4.8148 - learning_rate: 0.0100\n",
            "Epoch 6/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - loss: 4.7717 - val_loss: 4.7938 - learning_rate: 1.0000e-03\n",
            "Epoch 7/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 4.7594 - val_loss: 4.6633 - learning_rate: 1.0000e-03\n",
            "Epoch 8/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 4.6406 - val_loss: 4.6222 - learning_rate: 1.0000e-03\n",
            "Epoch 9/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 4.6019 - val_loss: 4.6017 - learning_rate: 1.0000e-03\n",
            "Epoch 10/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 46ms/step - loss: 4.5831 - val_loss: 4.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 11/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 45ms/step - loss: 4.5680 - val_loss: 4.5798 - learning_rate: 1.0000e-04\n",
            "Epoch 12/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 4.5604 - val_loss: 4.5791 - learning_rate: 1.0000e-04\n",
            "Epoch 13/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 4.5715 - val_loss: 4.5750 - learning_rate: 1.0000e-04\n",
            "Epoch 14/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 4.5556 - val_loss: 4.5764 - learning_rate: 1.0000e-04\n",
            "Epoch 15/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - loss: 4.5552 - val_loss: 4.5732 - learning_rate: 1.0000e-04\n",
            "Epoch 16/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - loss: 4.5605 - val_loss: 4.5734 - learning_rate: 1.0000e-05\n",
            "Epoch 17/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - loss: 4.5534 - val_loss: 4.5736 - learning_rate: 1.0000e-05\n",
            "Epoch 18/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 46ms/step - loss: 4.5547 - val_loss: 4.5732 - learning_rate: 1.0000e-05\n",
            "Epoch 19/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5545 - val_loss: 4.5733 - learning_rate: 1.0000e-05\n",
            "Epoch 20/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5590 - val_loss: 4.5732 - learning_rate: 1.0000e-05\n",
            "Epoch 21/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 4.5587 - val_loss: 4.5729 - learning_rate: 1.0000e-06\n",
            "Epoch 22/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - loss: 4.5584 - val_loss: 4.5728 - learning_rate: 1.0000e-06\n",
            "Epoch 23/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - loss: 4.5567 - val_loss: 4.5730 - learning_rate: 1.0000e-06\n",
            "Epoch 24/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - loss: 4.5612 - val_loss: 4.5729 - learning_rate: 1.0000e-06\n",
            "Epoch 25/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5593 - val_loss: 4.5729 - learning_rate: 1.0000e-06\n",
            "Epoch 26/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5582 - val_loss: 4.5730 - learning_rate: 1.0000e-07\n",
            "Epoch 27/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5568 - val_loss: 4.5730 - learning_rate: 1.0000e-07\n",
            "Epoch 28/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5624 - val_loss: 4.5730 - learning_rate: 1.0000e-07\n",
            "Epoch 29/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 4.5598 - val_loss: 4.5730 - learning_rate: 1.0000e-07\n",
            "Epoch 30/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 4.5574 - val_loss: 4.5730 - learning_rate: 1.0000e-07\n",
            "Epoch 31/98\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - loss: 4.5632 - val_loss: 4.5730 - learning_rate: 1.0000e-08\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799b0de9f6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # Convert to array and prevent numerical issues with very small numbers\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-7) / temperature  # Adjust by temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)  # Softmax\n",
        "    probas = np.random.multinomial(1, preds, 1)  # Sample from the softmax distribution\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "XTXOZwvknnur"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Prepare the decoder input for prediction\n",
        "decoder_input_test = np.zeros((noisy_x_test_padded.shape[0], max_length - 1), dtype=int)\n",
        "for i in range(len(x_test_padded)):\n",
        "    decoder_input_test[i] = x_test_padded[i, 1:]  # Drop the first word\n",
        "\n",
        "# Predicting denoised text\n",
        "denoised_texts = autoencoder.predict([noisy_x_test_padded[:10], decoder_input_test[:10]])\n",
        "\n",
        "# Function to convert sequences back to text\n",
        "def decode_denoised_sequence(sequence):\n",
        "    indices = tf.math.argmax(sequence, axis=1).numpy()\n",
        "    return ' '.join([reverse_word_index.get(word, '?') for word in indices])\n",
        "\n",
        "def decode_sequence_with_sampling(prob_distributions, temperature=1.0):\n",
        "    return ' '.join([reverse_word_index.get(sample(probs, temperature), '?') for probs in prob_distributions])\n",
        "\n",
        "# Decoding the denoised sequences\n",
        "for i in range(10):\n",
        "    print(\"Original:\", sequence_to_text(x_test[i]))\n",
        "    print(\"Noisy:\", sequence_to_text(noisy_x_test_padded[i]))\n",
        "    print(\"Generated:\", decode_sequence_with_sampling(denoised_texts[i], temperature=1))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "MKIxpqN-nliw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6a3a8b-00bc-407b-9803-1a9a989e0317"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Original: ? murder both in have <UNUSED> easily of of ? ? <UNK> <START> boring the <START> again ? understand dead <START> over a ? ? ? of of br how where first lead ? make you ? in have movie not ? are role dark and where in true director and old just <UNK> not last i lot ? an he film ? based both in <UNUSED> easily\n",
            "Noisy: ? murder both in have <UNUSED> easily of whether ? ? <UNK> <START> boring the <START> again that understand dead <START> more a ? ? ? of of br how where plus lead ? try that's ? in have movie not ? are role dark and where in true director and old just on not last i lot ? poor he ? ? based both in <UNUSED> easily ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "Generated: <START> no this emotional still and please <UNK> ok there bad also <UNK> br which in a theater and in the of on head <UNK> br became overall and at <UNK> wouldn't do a be little is character the say while were <UNK> <UNK> were and and it good need some girl is it case in great the people the and this and <UNK> this car on is you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "\n",
            "Original: ? in with ? <UNUSED> part the ? other is ? movie ? <UNK> watch directing <START> many a good score <UNK> films the <START> scene however set movie <START> find looking to ? ? <START> ? ? watch ? for <START> with ? and <UNUSED> ? ? of of <START> movies present all imagine ? was wait ? <UNK> ? ? or good isn't it novel ? eyes in ? <START> early <UNK> ? the not also characters better <START> ? ? ? early a ? end <UNK> has br any if which funny in with old a <START> horrible last one ? ? or perfect have the so ? movies for up watched is an mess <UNK> ? it or released lead ? know <UNUSED> some three this <START> ? world your ? ? ? lead ? about <UNUSED> wonderful going <START> ? ? looking ? <START> ? the <START> would good can to used in as a <UNUSED> casting ? the ? wait ? <UNK> ? his ? ? more <START> imagine <UNK> create four and <START> ? playing whole of of br ? novel ? in a have the not films characters it ? was ? ? decided fight in with a ? for or ? <UNK> or worst and ? <UNUSED> took forced ? they <START> directed it's film us and only all ? with out feel no man ? in a shows <START> very of of throughout or good subject and makes to truly ? film between you and weak is did <START> ?\n",
            "Noisy: <UNUSED> casting ? the ? wait ? <UNK> ? his ? ? more <START> imagine <UNK> create four and <START> ? playing new of of br ? novel ? in a have the not films characters leads ? was ? ? decided fight in with a fantasy for or ? <UNK> or worst and ? <UNUSED> took forced note they <START> directed it's film us and only all ? with out feel no man girls in a shows <START> very of of throughout or good subject and forward to note ? film between you and weak is did <START> ?\n",
            "Generated: is that <UNK> <UNK> the by <UNK> into ok for one <UNK> <UNK> <UNK> seems <UNK> of about an a a was drama except in character is your br <UNK> very is it's with to br the <UNK> remember meets over made br at star the the special <UNK> characters is script <UNK> not society movies i original a society which just it's i'm type that <UNK> to br it's <UNK> your performance obviously credits a <UNK> <UNK> <UNK> least were year suspense too even got happened for of <UNK> good more it's this <UNK> movie you involved to <UNK>\n",
            "\n",
            "\n",
            "Original: ? films richard ? ? ? ? <START> people ? ? the have during ? the <START> works ? richard even are ? role call ? <UNK> this is particularly role ? this <UNUSED> world your ? i there dream to ? were <START> went for not ? wouldn't and with think ? ? return ? and all ? ? to with music what are ? are make ? ? one ? ? ? <UNK> ? then have ? be <UNUSED> very in ? ? and <START> kids the ? ? try used <START> ? ? even are him to ? to ? be <UNUSED> very what ? white richard the he ? i to most ? of of <START> pay ? the <START> ? ? ? ? and sequences and ? best have the not into ? ? i <UNUSED> laugh pay ? ? stop this ? from <START> ? by 4 <UNUSED> thriller <UNUSED> ? the war without see his woman to role he the characters for end is a good laugh parents ? there be ? ? by see ? <START> ? ? were wanted by she because ? makes <START> ? ? through opinion ? <UNK> woman is want ? city <UNK> parents ? the <START> with ? that david ? follow the <START> ? their either yes ? ? was ? <UNK> ? to husband and father ? <START> ? ? the <START> ? of of having hate a looking to <UNUSED> ? <START> ? ? best <START> ? on ? was story ? <UNK> you mr <UNUSED> during ? and only if at see bad and ? ? at ? and ? ? that <UNUSED> thriller <START> ? on ? ? to in usually miss is liked you than named and its an films ? ? actually ? ? these ? that not going a ? this <UNUSED> ? title ? script to <START> ? by trying who ? am his change <UNUSED> ? what ? ? then <UNUSED> ? who roles by ? then <START> ? ? it in a when ? <UNK> ? <UNUSED> looks thriller ? ? ? then <UNUSED> ? of of ? was in ? believe before thriller by ? message then <START> ? ? great was <UNUSED> ? the ? ? ? <START> ? ? writers and turns great <START> ? and <START> ? this ? ? ? <START> ? and ? seems <START> ? ? ? <START> ? <UNK> don't screen movie <UNUSED> ? ? ? ? be ? <UNUSED> yes ? ? ? ? the <START> ? ? despite <START> ? what <START> real ? ? ending there ? ? that father are ? not ? <UNK> ? story ? was ? ? <START> ? the having would ? one <UNUSED> ? ? like actors are a ? a mean can fact living of of ? <START> very dialog what in ? plot with i him <UNK> ? <START> horror it is i him one <UNUSED> ? ? be <START> ? the it ? ? ? wife is she his named and only in this <UNUSED> ? her back apparently ? between from between but you really ? ? to nice what are him ? for hear ? not ? the soundtrack <START> with ? this <UNUSED> ? hate the ? ? ? make his <START> ? ? ? ? from ? to <START> ? from them non the them fails it ? most ? cast <UNK> a ? one ? or <UNUSED> ? with no <UNUSED> ? have to most ? made for most within a when apart\n",
            "Noisy: <START> ? the it ? ? ? wife is she his named particularly only in this <UNUSED> ? her back apparently ? between from between but you really ? ? to nice what yes him ? for hear ? not ? the soundtrack role with ? this <UNUSED> ? hate the ? ? ? make avoid <START> ? movies ? ? nothing ? to shot ? from them non the them fails it sense most ? cast <UNK> a ? one ? or <UNUSED> ? with no <UNUSED> ? have to most ? made for most within a when apart\n",
            "Generated: the people <UNK> over question a <UNK> gave <UNK> about work look <UNK> movie just the book he's he when the might film the they they every <UNK> the and <UNK> <UNK> <UNK> br <UNK> trying exactly else the in out top <UNK> another br her be the actor to a only he <UNK> her the well <UNK> <UNK> <UNK> the place top to she or the but by of ways but over will <UNK> such <UNK> was them <UNK> <UNK> very <UNK> that <UNK> do it in think <UNK> an <UNK> is there br the its the mr it's\n",
            "\n",
            "\n",
            "Original: ? br ? acting in except the as around in very br woman city ? and ? <START> course minutes br got bad it br been like ? her is in i supposed ? <START> down it s was <START> given need on good somewhat for <START> ? do on ? ? br 2 <UNUSED> ? your want can't <UNK> br see ? city can around br she but bad left <START> expect <START> plot rather know to in as get <START> oh to in as a ? was so wanted and <START> always the ? br happy br true an ? her so <UNK> so ? other br she you superb they're <START> child if <UNUSED> hell up i however time ? <UNK> television writers go reason film only so bad hollywood <START> does get a played simply terrible ? the i'm too it on season an laugh ? have see girlfriend only if a character season this all john me on ? happens was <START> ? movie <START> course an don't say <START> simple got me you audience ?\n",
            "Noisy: <START> oh to in as ever ? was so wanted excellent <START> always the ? br happy br true an ? her so <UNK> so ? other br she you superb they're <START> child if <UNUSED> hell using i however stuff ? <UNK> television perfectly go reason film only so bad hollywood <START> trying get a played simply terrible ? the i'm too it on season an you ? have see girlfriend sound if a character season this all john me total ? happens was <START> ? movie <START> course an don't say should simple got kind you audience ?\n",
            "Generated: the to and the of without the <UNK> couple turns <UNK> and after father <UNK> begins it it dance idea being subject hour of can the the isn't for an so himself <UNK> the nearly better and character away i plays <UNK> they played thing interesting please and <UNK> play <UNK> and <UNK> near it show a this us on over romantic however of in star sometimes <UNK> on is i when don't <UNK> <UNK> other to as <UNK> <UNK> die i out <UNK> because to the <UNK> <UNK> <UNK> he's <UNK> to would all feel <UNK> their them a\n",
            "\n",
            "\n",
            "Original: ? who out also do ? through <UNUSED> working day ? else <UNK> br overall in tries of of in tries favorite ? performances for ? had or shows is this genre this film remember ever ? <START> ? on girl <UNK> ? at been ? film ? ever nice able and ? through but ? in tries a get ? <UNK> a ? interesting of of and written in ? nature br you and written time ? ? her ? for murder top in tries ? his shows is of of would tale tale may thing play or it some ? thing four ? thing eventually thing\n",
            "Noisy: working day ? else exactly br overall in tries of expected in tries favorite problem performances for ? had or shows is this genre this film remember ever ? <START> ? on girl <UNK> ? at been ? film ? ever nice able and ? through comment ? in tries a get ? <UNK> a ? interesting of of and written in ? nature powerful you and written time ? ? her ? for murder top in tries ? his shows is of of near tale tale night thing play or eyes some ? thing four ? thing eventually thing\n",
            "Generated: you it's <UNK> and of you have br opinion saying liked are <UNK> you movie on any give that terrible are i know at ending movie basically but beginning a <UNK> br does this not <UNK> a to you can't this <UNK> <UNK> even on <UNK> fine <UNK> is bad in wife pay <UNK> this in it's not take class the but all <UNK> where light must <UNK> of br br years is human br <UNK> a it's need certainly br br right life i direction it a it strong bad oh <UNK> br it 10 of young are br\n",
            "\n",
            "\n",
            "Original: ? through supposed ? in as must character ? he by acting in as such ? directors <UNK> ? <START> ? that is ? learn you and ? is don't turn weak ? that ? br where reason <UNK> ? to in as love <UNUSED> some three <UNK> br ways ? just say directors ? place in as movie everything from movie ? to ? ? be saw br ways two them ? in a <UNUSED> ? some as <UNK> such his two one he <START> need to <START> before ? how much and only is <UNK> br any at such is such be saw his place something movie <START> ? in as watching ? <UNUSED> earth ? is ? <START> should doesn't through then real in as been his movie everything\n",
            "Noisy: no that instead br where reason <UNK> ? to in as love <UNUSED> even three <UNK> br acted ? just say directors ? place in as movie everything from movie eventually to ? ? be saw br ways two them ? in a <UNUSED> ? some as <UNK> such his two one he <START> need to <START> before ? how much and only is <UNK> br any at such is such be saw his leading something movie <START> ? in as watching ? <UNUSED> earth ? is ? <START> should doesn't through then real in as been his movie everything\n",
            "Generated: heard an i had even so i of to <UNK> think since have <UNK> i up you far i i really it's real those nothing there <UNK> for it's quality you <UNK> <UNK> liked they about is it its becomes it <UNK> up film film <UNK> this i'm did all movie <UNK> ending if stars your to well this movies i guess but is out you just all movie episode at it another <UNK> ever this i i it young me it it to lines miss don't disappointed bad but <UNK> just years an <UNK> french even a have just\n",
            "\n",
            "\n",
            "Original: ? ? felt and his like <UNUSED> going the <UNUSED> ones ? <START> dvd ? ? <START> ? the ? ? a <START> would the <START> ? the <START> ? the <START> ? to ? ? <START> with ? was <START> ? ? ? <UNK> <START> ? ? <START> ? the <START> all ? said set high even <START> ? ? a anyway in ? a running and <START> ? and ? to ? what <START> ? ? movie <UNUSED> ones ? the ? ? <START> ? ? ? admit <UNK> ? cast <START> ? ? more <START> ? <UNUSED> ? ? a ? and ? <START> ? for be <START> god what <START> ? point will ? story ? ? <START> and ? go ? british ? ? book a earlier ? ? ? it in i <START> ? the <START> 1 ? ? it she ? to ? <START> with a ? will important won't short <UNK> ? several movie <START> all ? they <START> given <START> ? ? <UNK> ? <START> ? of of ? i <UNUSED> ? ? for be <START> ? body but ? and its <UNUSED> ? several ? completely ? ? and both <START> with <UNUSED> ? actually an it <START> directed lady are a ? movie <UNUSED> ? <UNK> ? ? would in ? a completely one <START> christmas the ? of of ? ? ? movie from <START> business the british ? by really ? ? ? <START> ? ? the <START> again on if have ? they ? in ? a 1 completely one ? ? to hope ? screen and enjoyed <UNK> memorable instead ? for to ? when have ? a again this <UNUSED> scary from ? <START> would a extremely go <UNUSED> that's the scene it on ? to <UNUSED> during result stop this ? <START> sister <UNK> ? the used ? and ? <UNUSED> ? result movie <START> directed ? ? ? get completely <START> ? for when have ? is an can this ? of of <START> ? ? the <START> ? ? to story a ? one <START> ? ? the ? ? they ? to <START> ? to ? <START> ? thought <START> ? <UNK> story ? a ? was ? <UNK> <START> ? ? the <START> ? ? story ? was ? of of ? ? <START> ? the viewers all may <UNK> ? is few for they <UNUSED> ? looks ? and ? ? <START> ? the <UNUSED> ? ? <START> problems both every in tv having ? ? ? <START> fans the <START> does in ? a completely to anything dvd be ? get what <START> ? ? a ? little <START> behind not ? ? they <START> ? is i still ? it <START> ? completely and ? <UNK> ? <START> ? ? ? in miss ? <START> ? and <START> ? the <START> ? fails of of <START> because ? miss to <START> with <UNK> stupid to he the with children a <START> ? ? <START> ? ? the <START> ? on ? was ? the ? ? <UNK> shame writer a i've ? <UNK> but ? kind <START> ? ? the <START> ? the <UNUSED> look been anything to <START> ? the <START> directed ? of of <START> ? talking the ? ? <UNK> ? ? <START> ? on ? ? was take ? ? they <UNUSED> ? be <START> line the <START> ? <START> ? the <START> miss a ? ? thought <START> ? ? <UNK> <UNUSED> look ? by ? more ? and ? then and violence <UNUSED> michael more the <UNUSED> ? episode ? to seems could character ? one <UNUSED> ? ? a <START> ? ? the <START> ? ? to ? to ? <UNK> style <UNUSED> ? ? the <START> seems the <START> ? ? of of <START> poor the <UNUSED> world oh viewing and <UNUSED> hot ? ? cast <START> ? to <UNUSED> miss it about than ? one ? to ? ? one ? ? to ? <UNK> ? ? to <START> ? in miss a possible ? they dumb ? ? ? just if performance i my <UNUSED> important mind ? of of ? a <UNUSED> with it <START> ? ? ? is that back 5 ? <UNK> is that <START> ? is ? the personal <UNK> ? was <START> ? ? the <START> ? ? ? most t ? about than movie with ? by you ? <UNK> my ? ? movie ? ? to ? loved ? happen\n",
            "Noisy: <UNUSED> miss question about herself ? one ? to ? ? one ? ? to own <UNK> ? ? to <START> ? in miss a possible none they dumb ? ? ? eyes if performance i my <UNUSED> important mind ? of of ? a <UNUSED> with it <START> ? ? ? is that back drama turned <UNK> is that <START> ? is ? got personal <UNK> ? was <START> there ? the <START> ? ? cinematography most t james about than movie with comment by you ? <UNK> song ? ? movie ? ? to ? loved ? happen\n",
            "Generated: and minutes set released by the <UNK> street <UNK> <UNK> decided also the shots finally br <UNK> run with <UNK> <UNK> <UNK> ever <UNK> york <UNK> <UNK> a entertaining a <UNK> its to the the the and with <UNK> all <UNK> br movies points you in lead years the horror <UNK> who in <UNK> such and who a to many <UNK> everything <UNK> and <UNK> their years <UNK> this of turn <UNK> american br do it did into big shows <UNK> romantic and <UNK> believable <UNK> think few br have <UNK> the <UNK> eyes it the as <UNK> <UNK> it's\n",
            "\n",
            "\n",
            "Original: ? <START> ? george ? meet a ? and ? ? ? meet around what ? ? ? ? to enjoyed and ? <UNUSED> ? ? and <START> ? not meet a ? one ? meet could <UNUSED> ? ? best ? a ? they enjoyed <UNUSED> ? ? it ? meet comedy ? meet an it up see ? so ? ? in a ? <UNK> <START> ? sorry to acting an bad ? <UNK> ? <START> boring the <START> with ? one was killing <UNK> be <START> does ? meet style ? for by a <START> second of of <START> meet would a <START> good form ? it a completely and top <UNK> present <UNUSED> would thought ? most <UNUSED> understand release has on help worth exactly ? he the even on ? we events <UNK> ? ? most like all known performances with ? about way films problems to hard ? <UNK> ? then to all role ? ? most but another show for most background and his ? <UNK> ? you interesting well off were in\n",
            "Noisy: <START> with ? one was killing <UNK> be <START> does ? meet style ? for by a <START> second thinking of <START> meet fire a <START> good form ? it a completely and top <UNK> present <UNUSED> would thought ? most <UNUSED> understand release has on help worth exactly ? we the even on ? perfectly events <UNK> ? ? most like all known performances with ? sci way films problems to hard ? <UNK> ? then to all role is ? most but another show for most background and his gives <UNK> ? you interesting well off were always\n",
            "Generated: the basically <UNK> <UNK> <UNK> the but the come the through <UNK> been <UNK> <UNK> <UNK> own most also she in a <UNK> <UNK> <UNK> is at but what <UNK> an <UNK> <UNK> that also about who <UNK> the the also the as well in in <UNK> of can't performances i is <UNK> in believe older is and story <UNK> he made sort <UNK> movies of gone a him the not are <UNK> good since rather for and to <UNK> <UNK> all is <UNK> <UNK> <UNK> the is 2 <UNK> but the you like to watching <UNK> and <UNK> who\n",
            "\n",
            "\n",
            "Original: ? rest really <UNUSED> take acting ? was ? ? ? ? for look the still ? you ? <START> ? the very <START> because let's their <START> ? ? ? ? characters even you take minutes itself ? in have a ? was ? events ? ? <UNK> ? or <UNUSED> full ? the ? <UNK> want animation ? opening is i ? that most ? during plays even on role ? to in tv <UNK> killed ? ? the ? ? <START> my ? ? stay even ? a most takes ? <UNK> ? annoying and man the <START> films characters him to in flick look the then point more and ? ? series ? the ? role them also ? ? with a ? and in have those or <UNUSED> ?\n",
            "Noisy: you take minutes itself beginning in have a finds was ? events ? ? <UNK> ? word <UNUSED> full ? the ? <UNK> want animation ? opening is i ? that most thinking during plays even on role ? children in tv <UNK> killed better ? the ? ? <START> my ? ? stay even ? a had takes ? <UNK> ? annoying and man the <START> films characters america to in far look the then point more and ? ? series ? the ? role them also ? eyes with a ? and in have those or <UNUSED> ?\n",
            "Generated: until feel film poorly not at <UNK> that could <UNK> laughs we out back roles him <UNK> the that been him and very better this they <UNK> given in <UNK> actors be high real time <UNK> use <UNK> <UNK> br i of half for for both not history the i'll with <UNK> to was drama can after br little we however her of a but but br to actors <UNK> what <UNK> it's films <UNK> a that if to if is will films <UNK> it her to predictable no brings get <UNK> about gore it me not see <UNK> up\n",
            "\n",
            "\n",
            "Original: ? in with a best <START> ? ? ? and you ? those br been ? it <START> ? guys ? ? their special <UNK> make you than king ? one <UNUSED> ? nothing this is talent just in i but and his <START> finally of of word ? ? ? their modern was first laugh in mind ? with they ? ? talent just monster they <START> ? the ? far simply they ? <UNK> <START> ? ? the story ? ? and seemed <START> ? will all ? and <START> ? away that's br ? and moments first ? ? see his ? be he ? of of once is a but lot <START> ? the <START> into with those is ? he <START> ? ? the early woman to ? <UNUSED> ? whole it ? <START> ? thought but like think for wasn't movies ? ? ? and <START> ? ? the ? <UNK> words ? this <START> ? to <START> into with kind that he in is ? <UNUSED> ? laugh with once and <START> made <START> ? i i've background and his <UNK> decent ? of of lead ? ? this <START> ? <UNK> not ? ? ? ? was ? ? ? ? ? surprised to ? the <START> ? ? ? sense not ? to somehow ? ? <START> ? ? ? to horrible ? ? the ? ? george ? <UNK> ? ? <START> ? ? ? sounds ? ? long this seem <UNK> this <START> ? <START> single ? seemed to special dead anyone ? <UNK> ? these job ? <UNK> ? ? <START> ? behind the ? ? of of has on out ? ? to in with her <START> my ? br see which always just a <UNUSED> period and <START> whole to out ? even br any a disappointed huge and <START> made in with a <UNUSED> single ? highly has black must fact very and ? ? showing director movie of of ? in a <UNUSED> ? some with br decent sex i'm in to ? was <START> into <UNK> don't ? that first some <START> that's make you than really is ? friend ? <UNK> ?\n",
            "Noisy: ? ? <START> ? behind the ? ? of of has on out ? ? to in with her <START> my ? br see which always just a <UNUSED> period and <START> whole to out ? even br scene a disappointed huge and <START> made in with famous <UNUSED> single ? highly has black must fact very and ? ? showing director movie towards of ? in man <UNUSED> ? some with br decent sex i'm in to ? was <START> happens <UNK> don't ? that first some both that's make you than really is ? friend ? <UNK> ?\n",
            "Generated: <UNK> even when the <UNK> avoid true film <UNK> the took <UNK> there's instead those can't the the it <UNK> three written you an wouldn't br well before <UNK> couple to times monster i watch enough <UNK> is can this is <UNK> the <UNK> movie watch about that's <UNK> of the <UNK> about up than <UNK> enough starts here also <UNK> who it's movie of these characters is other involved you're the otherwise it will of hope was this place want <UNK> saw would through perhaps funny is lack <UNK> so but the make be watch also this br a\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BofMuoMEntRt"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}