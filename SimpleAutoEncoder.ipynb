{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000  # Size of the vocabulary\n",
        "max_length = 150    # Maximum length of input sentences\n",
        "latent_dim = 64     # Size of the latent space\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "epochs = 40        # Number of training epochs"
      ],
      "metadata": {
        "id": "oAm4FBuHzOAU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset\n",
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=vocab_size)\n",
        "x_train = pad_sequences(x_train,maxlen=max_length,padding='post')  # Pad the sequences to the right to have a tensor of shape (batch_size, max_length)\n",
        "x_test = pad_sequences(x_test,maxlen=max_length,padding='post')   # Pad the sequences to the right to have a tensor of shape (batch_size, max_length)"
      ],
      "metadata": {
        "id": "AEbLuMvgBsFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5865d509-984d-4f7f-a2ca-42157670b2eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB word index\n",
        "word_index = imdb.get_word_index()\n",
        "# Reverse the word index to map integer indices to words\n",
        "reverse_word_index = {value:key for key,value in word_index.items()}\n",
        "# Add padding, start, and unknown tokens\n",
        "reverse_word_index[0] = '<PAD>'\n",
        "reverse_word_index[1] = '<START>'\n",
        "reverse_word_index[2] = '<UNK>'\n"
      ],
      "metadata": {
        "id": "h8lKck1cBuKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9fa3be-6d59-4849-ac95-9669c2b4035b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "inputs = Input(shape=(max_length,))\n",
        "embedded = Embedding(vocab_size,embedding_dim)(inputs)  # Embedding layer\n",
        "flattened = Flatten()(embedded)  # Flatten the output of the embedding layer\n",
        "encoded = Dense(latent_dim, activation='relu')(flattened)  # Dense layer with latent_dim\n",
        "encoder_model = Model(inputs, encoded)  # Encoder model\n"
      ],
      "metadata": {
        "id": "ca8Oy2m0BwHW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decoder\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "reconstructed = Dense(max_length*embedding_dim,activation='relu') (latent_inputs) # Dense layer with max_length*embedding_dim\n",
        "reshaped = Reshape((max_length,embedding_dim))(reconstructed)  # Reshape the output of the Dense layer to (max_length, embedding_dim)\n",
        "decoded = Dense(vocab_size,activation='softmax')(reshaped)  # Dense layer with vocab_size as last layer\n",
        "decoder_model = Model(latent_inputs, decoded)  # Decoder model\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Model(inputs,decoder_model(encoder_model(inputs))) # Autoencoder model\n"
      ],
      "metadata": {
        "id": "lK4HYe3yByZC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')   #  actual loss for the autoencoder model\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(x_train,x_train,epochs=epochs,batch_size=32,validation_data=(x_test,x_test))"
      ],
      "metadata": {
        "id": "iMJI2gaRB0Jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107717ae-307f-4dda-f91d-6db8de756c08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - loss: 4.7616 - val_loss: 5.2356\n",
            "Epoch 2/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 23ms/step - loss: 4.7119 - val_loss: 5.2929\n",
            "Epoch 3/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6967 - val_loss: 5.3152\n",
            "Epoch 4/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6797 - val_loss: 5.3390\n",
            "Epoch 5/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.6656 - val_loss: 5.3499\n",
            "Epoch 6/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.6589 - val_loss: 5.3790\n",
            "Epoch 7/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6518 - val_loss: 5.3917\n",
            "Epoch 8/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6396 - val_loss: 5.4073\n",
            "Epoch 9/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6408 - val_loss: 5.4271\n",
            "Epoch 10/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.6351 - val_loss: 5.4417\n",
            "Epoch 11/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - loss: 4.6347 - val_loss: 5.4597\n",
            "Epoch 12/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - loss: 4.6243 - val_loss: 5.4754\n",
            "Epoch 13/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.6273 - val_loss: 5.4817\n",
            "Epoch 14/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.6337 - val_loss: 5.5045\n",
            "Epoch 15/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.6277 - val_loss: 5.5206\n",
            "Epoch 16/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.6165 - val_loss: 5.5306\n",
            "Epoch 17/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.5883 - val_loss: 5.5435\n",
            "Epoch 18/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - loss: 4.5997 - val_loss: 5.5634\n",
            "Epoch 19/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5965 - val_loss: 5.5779\n",
            "Epoch 20/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5948 - val_loss: 5.5915\n",
            "Epoch 21/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5808 - val_loss: 5.5996\n",
            "Epoch 22/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5807 - val_loss: 5.6208\n",
            "Epoch 23/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5754 - val_loss: 5.6290\n",
            "Epoch 24/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5872 - val_loss: 5.6498\n",
            "Epoch 25/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5764 - val_loss: 5.6646\n",
            "Epoch 26/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.5730 - val_loss: 5.6738\n",
            "Epoch 27/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5652 - val_loss: 5.6845\n",
            "Epoch 28/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.5656 - val_loss: 5.7076\n",
            "Epoch 29/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5625 - val_loss: 5.7130\n",
            "Epoch 30/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 4.5584 - val_loss: 5.7214\n",
            "Epoch 31/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5666 - val_loss: 5.7390\n",
            "Epoch 32/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5675 - val_loss: 5.7414\n",
            "Epoch 33/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5486 - val_loss: 5.7629\n",
            "Epoch 34/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5496 - val_loss: 5.7696\n",
            "Epoch 35/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5535 - val_loss: 5.7870\n",
            "Epoch 36/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5685 - val_loss: 5.7974\n",
            "Epoch 37/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5414 - val_loss: 5.8099\n",
            "Epoch 38/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - loss: 4.5551 - val_loss: 5.8237\n",
            "Epoch 39/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5510 - val_loss: 5.8331\n",
            "Epoch 40/40\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 4.5390 - val_loss: 5.8401\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c75225fe510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example (after training)\n",
        "decoded_texts = autoencoder.predict(x_test[:10])  # AutoEncode the first 10 texts\n",
        "print(decoded_texts)"
      ],
      "metadata": {
        "id": "Ry-bV1t5zUAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6c7a87-09c0-40e1-c7d8-a9a329e48277"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "[[[4.65756131e-07 9.99999523e-01 5.48204981e-10 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.20169715e-29]\n",
            "  [7.08906214e-07 2.12997367e-43 2.10322614e-04 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.97733624e-34]\n",
            "  [2.12311375e-06 1.10919104e-19 4.80970815e-02 ... 2.93659478e-30\n",
            "   1.53903027e-13 1.33900154e-27]\n",
            "  ...\n",
            "  [9.99999642e-01 0.00000000e+00 9.03732547e-13 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [1.00000000e+00 0.00000000e+00 1.15695277e-16 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [1.00000000e+00 0.00000000e+00 3.73937142e-20 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            " [[2.31214173e-10 7.57055823e-05 8.20362568e-02 ... 1.26301579e-22\n",
            "   3.64609124e-12 2.89796063e-08]\n",
            "  [3.10789368e-08 1.30732068e-13 1.37516335e-01 ... 1.76072226e-10\n",
            "   1.28211929e-11 2.34959496e-10]\n",
            "  [2.91504221e-09 2.37055036e-20 1.08677752e-01 ... 6.77242132e-19\n",
            "   3.84878879e-31 9.57885324e-18]\n",
            "  ...\n",
            "  [1.02670401e-05 6.63394062e-11 4.02278639e-02 ... 8.25266742e-16\n",
            "   1.00846462e-15 6.77367052e-25]\n",
            "  [8.86515323e-08 6.61263195e-18 9.21881199e-02 ... 3.03552190e-17\n",
            "   6.33807920e-14 1.09879651e-12]\n",
            "  [3.53222754e-06 9.33941577e-15 9.64767784e-02 ... 3.71844310e-19\n",
            "   6.23279340e-40 4.81928205e-18]]\n",
            "\n",
            " [[1.03193266e-10 1.33759936e-03 3.41867916e-02 ... 6.26272752e-18\n",
            "   6.49717769e-23 9.31567592e-06]\n",
            "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [3.63962072e-12 1.32047284e-20 2.68431753e-02 ... 2.80259693e-45\n",
            "   9.26544775e-22 4.98090284e-13]\n",
            "  ...\n",
            "  [2.57119837e-07 9.80297911e-16 4.57126610e-02 ... 4.91001549e-18\n",
            "   1.04822592e-18 9.14870003e-12]\n",
            "  [2.58617973e-07 5.11651286e-18 1.06792018e-01 ... 2.31755485e-23\n",
            "   6.61402541e-22 3.55846019e-09]\n",
            "  [4.09249924e-06 1.40889763e-12 1.19677670e-01 ... 7.17455267e-24\n",
            "   2.54395809e-31 3.28379495e-18]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.86163156e-08 1.25879038e-03 1.15980975e-01 ... 2.44864644e-25\n",
            "   9.19885695e-13 3.94341976e-10]\n",
            "  [1.46143249e-08 1.45116984e-22 5.08821569e-03 ... 8.08918176e-39\n",
            "   2.91677733e-28 2.77010681e-09]\n",
            "  [1.09714295e-11 3.55470590e-13 1.57346442e-01 ... 1.19206824e-20\n",
            "   4.82841074e-07 9.75421201e-19]\n",
            "  ...\n",
            "  [1.66405190e-03 6.47332936e-19 3.91268879e-02 ... 6.08962087e-21\n",
            "   1.61123925e-36 1.79608592e-10]\n",
            "  [2.24488997e-03 1.43233879e-24 5.07888803e-03 ... 6.20048796e-34\n",
            "   4.44504049e-32 4.67265728e-08]\n",
            "  [3.64757851e-02 6.31421456e-37 7.82206096e-03 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.68691554e-24]]\n",
            "\n",
            " [[4.37243735e-08 1.00000000e+00 7.87018367e-12 ... 0.00000000e+00\n",
            "   0.00000000e+00 7.03311699e-42]\n",
            "  [6.19566754e-06 2.50185402e-28 8.00912380e-02 ... 0.00000000e+00\n",
            "   1.17995012e-20 1.45840927e-31]\n",
            "  [2.48778989e-08 1.73932756e-18 3.31236096e-03 ... 0.00000000e+00\n",
            "   5.14401970e-31 2.58575354e-26]\n",
            "  ...\n",
            "  [1.00000000e+00 0.00000000e+00 3.31855284e-13 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [1.00000000e+00 0.00000000e+00 1.24101543e-17 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [1.00000000e+00 0.00000000e+00 1.22734916e-23 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            " [[2.16218668e-10 2.80383509e-03 1.12497240e-01 ... 2.35948827e-20\n",
            "   2.44420640e-14 2.15434795e-07]\n",
            "  [2.21993930e-21 0.00000000e+00 2.18670009e-27 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.18409720e-42]\n",
            "  [1.31806079e-12 1.58985882e-18 4.40207496e-02 ... 0.00000000e+00\n",
            "   1.36205402e-25 2.49026942e-18]\n",
            "  ...\n",
            "  [4.13426562e-07 7.07241488e-10 5.83656095e-02 ... 1.43442411e-11\n",
            "   2.88428695e-16 1.70674191e-14]\n",
            "  [2.94487819e-07 6.30250812e-17 7.75931776e-02 ... 4.44886399e-28\n",
            "   2.31263080e-29 1.90844858e-07]\n",
            "  [7.70501003e-07 1.18059935e-14 9.68549326e-02 ... 8.81612326e-16\n",
            "   8.16050125e-28 1.12589174e-17]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(sequence):\n",
        "    \"\"\"Decode a sequence of integers back to words.\"\"\"\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "\n",
        "# Assume `decoded_texts` is the output from the decoder and get the argmax\n",
        "decoded_sequences = np.argmax(decoded_texts,axis=-1)\n",
        "\n",
        "# Convert each sequence in the decoded_sequences back to text\n",
        "decoded_texts = [decode_sequence(seq) for seq in decoded_sequences]\n",
        "\n",
        "# Example: print the first decoded text\n",
        "print(decoded_texts[0])\n"
      ],
      "metadata": {
        "id": "Y78OsRn0zYuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d4b27-4d35-47c2-eba6-6705eb5a5951"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "? this is this movie a movie ? it ? ? in <START> ? of <START> film ? ? ? <START> ? in ? ? ? br ? is some ? but is ? is ? ? this ? in this movie ? br manages to ? as is supposed to be ? <UNK> his ? is is ? that ? this behaves ? ? <UNK> a strongest ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xaLTDm72zjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}