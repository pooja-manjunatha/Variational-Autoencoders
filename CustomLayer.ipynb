{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers"
      ],
      "metadata": {
        "id": "qxIJkTKx-3qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Keras model with multiple outputs and a custom sampling layer, similar to those used in Variational AutoEncoders (VAEs), is an interesting task. Additionally, we'll incorporate custom loss functions for each output. Here's how we can achieve this:\n",
        "\n",
        "Here's how we can do it:\n",
        "\n",
        "1. Define the Custom Sampling Layer: This layer will be used to sample from a probability distribution, typically used in VAEs.\n",
        "\n",
        "1. Define the Model Architecture: The model will include the custom sampling layer and have two outputs.\n",
        "\n",
        "1. Implement Custom Loss Functions:\n",
        "        - Custom Binary Classification Loss: A simple example could be a variant of binary cross-entropy.\n",
        "        - KL Divergence Loss: TensorFlow provides a function for KL Divergence, which we could use directly but in this case we will provide a variation\n",
        "\n",
        "1. Generate Dummy Data: We'll create dummy data appropriate for our model's input and output specifications.\n",
        "\n",
        "1. Compile and Train the Model: We'll compile the model with our custom loss functions and then train it.\n",
        "\n"
      ],
      "metadata": {
        "id": "6iKEdMsL-69n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import KLDivergence\n",
        "import numpy as np\n",
        "\n",
        "# 1. Custom Sampling Layer\n",
        "\n",
        "class SamplingLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs  # Extract z_mean, z_log_var\n",
        "        batch = tf.shape(z_mean)[0]  # Calculate batch_size from the shape of z_mean\n",
        "        dim = tf.shape(z_mean)[1]   # Calculate dimension  from the shape of z_mean\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean+tf.exp(0.5*z_log_var)*epsilon   # Return the sampled data z_mean + e^(0.5*z_log_var)*epsilon\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZJA14-H-6Yu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Model architecture\n",
        "input_layer = Input(shape=(10,))\n",
        "dense_layer = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# For the VAE-like output\n",
        "z_mean = Dense(10)(dense_layer)  # Dense layer with 10 neurons\n",
        "z_log_var = Dense(10)(dense_layer)   # Dense layer with 10 neurons\n",
        "sampling_output = SamplingLayer()([z_mean,z_log_var])  # Custom sampling layer\n",
        "output1 = Dense(1,activation=\"sigmoid\",name=\"output1\")(dense_layer)  # Binary classification output from dense_layer\n",
        "output2 = Dense(5,activation=\"softmax\",name=\"output2\")(sampling_output)  # Multiclass classification output using the sampled data\n",
        "\n",
        "model = Model(inputs=input_layer,outputs=[output1,output2])  # Create the model"
      ],
      "metadata": {
        "id": "0uaLqKFxCtDH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Custom binary classification loss\n",
        "def custom_binary_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    epsilon = 1e-15\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "\n",
        "# 3. Custom KL Divergence Loss\n",
        "def custom_kl_divergence_loss(y_true, y_pred, scale_factor=1.0):\n",
        "    kl_loss = tf.keras.losses.KLDivergence()(y_true, y_pred)\n",
        "    return scale_factor * kl_loss\n"
      ],
      "metadata": {
        "id": "ird5Kzba_x_V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Generate dummy data\n",
        "x_dummy = np.random.random((1000, 10))\n",
        "y_dummy_output1 = np.random.randint(2, size=(1000, 1))\n",
        "y_dummy_output2 = np.random.randint(5, size=(1000, 5))\n"
      ],
      "metadata": {
        "id": "7c_hK1DK_01U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'output1': custom_binary_loss, 'output2': lambda y_true, y_pred: custom_kl_divergence_loss(y_true, y_pred, scale_factor=2.0)},\n",
        "              metrics={'output1': ['accuracy'], 'output2': ['accuracy']})\n",
        "\n",
        "# 5. Train the model\n",
        "model.fit(x_dummy, {'output1': y_dummy_output1, 'output2': y_dummy_output2}, epochs=10)"
      ],
      "metadata": {
        "id": "KJ_Kw0cN_3ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d2c7ba-0a86-4588-aaff-bd5e7f90bc89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17.3628 - output1_accuracy: 0.5465 - output1_loss: 0.6890 - output2_accuracy: 0.1782 - output2_loss: 16.6752\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.6837 - output1_accuracy: 0.5196 - output1_loss: 0.6905 - output2_accuracy: 0.1875 - output2_loss: 14.9927\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5454 - output1_accuracy: 0.5475 - output1_loss: 0.6858 - output2_accuracy: 0.2146 - output2_loss: 13.8594\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1569 - output1_accuracy: 0.5137 - output1_loss: 0.6918 - output2_accuracy: 0.1750 - output2_loss: 13.4628\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.8859 - output1_accuracy: 0.4960 - output1_loss: 0.6932 - output2_accuracy: 0.1913 - output2_loss: 13.1921\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7591 - output1_accuracy: 0.5318 - output1_loss: 0.6917 - output2_accuracy: 0.2122 - output2_loss: 13.0679\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6487 - output1_accuracy: 0.5322 - output1_loss: 0.6906 - output2_accuracy: 0.1669 - output2_loss: 12.9580\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6025 - output1_accuracy: 0.5277 - output1_loss: 0.6901 - output2_accuracy: 0.1773 - output2_loss: 12.9135\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.5655 - output1_accuracy: 0.5226 - output1_loss: 0.6890 - output2_accuracy: 0.2066 - output2_loss: 12.8760\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6404 - output1_accuracy: 0.5354 - output1_loss: 0.6886 - output2_accuracy: 0.1870 - output2_loss: 12.9538\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a820309df90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a Keras model is trained, you can evaluate its performance on a test dataset and use it to make predictions. Continuing from the previous example, I'll show you how to:\n",
        "\n",
        "1. Evaluate the Model: We'll evaluate the model on a separate set of dummy data to see how it performs.\n",
        "\n",
        "2. Use the Model for Prediction: We'll use the model to make predictions based on new input data."
      ],
      "metadata": {
        "id": "i_x1B6c3_8W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Evaluate the model\n",
        "\n",
        "# Generate some dummy test data\n",
        "x_dummy_test = np.random.random((200, 10))\n",
        "y_dummy_test_output1 = np.random.randint(2, size=(200, 1))  # Binary labels\n",
        "y_dummy_test_output2 = np.random.randint(5, size=(200, 5))  # One-hot encoded labels for 5 classes\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation = model.evaluate(x_dummy_test, {'output1': y_dummy_test_output1, 'output2': y_dummy_test_output2})\n",
        "print(f\"Test Loss, Test Accuracy for Output 1: {evaluation[1]}, {evaluation[3]}\")\n",
        "print(f\"Test Loss, Test Accuracy for Output 2: {evaluation[2]}, {evaluation[4]}\")\n"
      ],
      "metadata": {
        "id": "dNf8dv9ZAB56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdd8ee9-c107-4a2c-cf68-57247d3a5f8a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.3491 - output1_accuracy: 0.5621 - output1_loss: 0.6829 - output2_accuracy: 0.1494 - output2_loss: 12.6845  \n",
            "Test Loss, Test Accuracy for Output 1: 0.6823347806930542, 0.5400000214576721\n",
            "Test Loss, Test Accuracy for Output 2: 12.636900901794434, 0.18000000715255737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Use the model for prediction\n",
        "\n",
        "# New sample data for prediction\n",
        "new_sample = np.random.random((1, 10))\n",
        "\n",
        "# Making predictions\n",
        "predictions = model.predict(new_sample)\n",
        "print(f\"Predictions for Output 1 (Binary classification): {predictions[0]}\")\n",
        "print(f\"Predictions for Output 2 (Multiclass classification): {predictions[1]}\")\n"
      ],
      "metadata": {
        "id": "22brxmbSAFmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e98e91-b16a-4e54-a505-39851ee86851"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "Predictions for Output 1 (Binary classification): [[0.5175721]]\n",
            "Predictions for Output 2 (Multiclass classification): [[0.18913573 0.23185126 0.1756104  0.1510117  0.25239086]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGxalfR4BAUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}